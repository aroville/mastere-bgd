{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from nltk.help import upenn_tagset\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from multiprocessing import Pool\n",
    "from normalizr import Normalizr\n",
    "from chronometer import Chronometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got texts in 0.174s\n2000 documents\n"
     ]
    }
   ],
   "source": [
    "def read_f(f):\n",
    "    return open(f).read()\n",
    "\n",
    "with Chronometer() as chrono:\n",
    "    # Load data\n",
    "    uri = '/home/axel/mastere/machine_learning/tpnote/data'\n",
    "    filenames_neg = sorted(glob(op.join(uri, 'imdb1', 'neg', '*.txt')))\n",
    "    filenames_pos = sorted(glob(op.join(uri, 'imdb1', 'pos', '*.txt')))\n",
    "    texts_neg = Pool().map(read_f, filenames_neg)\n",
    "    texts_pos = Pool().map(read_f, filenames_pos)\n",
    "    texts = texts_neg + texts_pos\n",
    "print('Got texts in {:.3f}s'.format(float(chrono)))\n",
    "\n",
    "y = np.ones(len(texts), dtype=np.int)\n",
    "y[:len(texts_neg)] = 0.\n",
    "\n",
    "print(\"%d documents\" % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got stop words in 0.000s\n"
     ]
    }
   ],
   "source": [
    "normalizr = Normalizr(language='en')\n",
    "normalizr_options = [\n",
    "    'remove_accent_marks',\n",
    "    'replace_hyphens',\n",
    "#     'replace_punctuation',\n",
    "    'replace_symbols',\n",
    "    'remove_extra_whitespaces'\n",
    "]\n",
    "\n",
    "with Chronometer() as chrono:\n",
    "    sw = open(uri + '/english.stop').read().splitlines()\n",
    "print('Got stop words in {:.3f}s'.format(float(chrono)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(ts):\n",
    "    return [normalizr.normalize(t, normalizr_options) for t in ts]\n",
    "\n",
    "def not_in_sw(w):\n",
    "    return w not in sw\n",
    "\n",
    "def filter_sw(split):\n",
    "    return list(filter(not_in_sw, split))\n",
    "\n",
    "def count_words(t, ignore_sw):\n",
    "    with Chronometer() as chrono:\n",
    "        splits = Pool().map(normalize, t)\n",
    "        if ignore_sw:\n",
    "            splits = Pool().map(filter_sw, splits)\n",
    "\n",
    "        d = {w: i for i, w in enumerate(set(np.concatenate(splits)))}\n",
    "\n",
    "        counts = np.zeros((len(t), len(d)))\n",
    "        for ix_text, split in enumerate(splits):\n",
    "            for word in split:\n",
    "                counts[ix_text, d[word]] += 1\n",
    "    print('Word count in {:.3f}s'.format(float(chrono)))\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2 Les classes positives et négatives ont été assignées à partir des notes données au film, avec une échelle différente selon le système de notation de la source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self):\n",
    "        self.prior = None\n",
    "        self.condprobe = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        Given x, a matrix of number of apparition by term and text, \n",
    "        and y, a vector containing the labels associated with each text,\n",
    "        calculate the frequency for each word, by label.\n",
    "        '''\n",
    "        with Chronometer() as chrono:\n",
    "            n_docs, n_words = x.shape\n",
    "            classes = np.unique(y)\n",
    "            p = len(classes)\n",
    "\n",
    "            # probability a priori\n",
    "            self.prior = np.empty(p)\n",
    "            self.condprobe = np.empty((p, n_words))\n",
    "\n",
    "            for i, c in enumerate(classes):\n",
    "                # over all the training data, frequency of label c\n",
    "                self.prior[i] = len(y[y == c]) / n_docs\n",
    "\n",
    "                # calculate the frequency of each word\n",
    "                t = np.sum(x[y == c], axis=0)\n",
    "                self.condprobe[i] = (t + 1) / np.sum(t + 1)\n",
    "            \n",
    "        print('Fit in {:.3f}s'.format(float(chrono)))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Calculates a score for each label for a new \"apparition matrix\"\n",
    "        Return the higher scoring labels\n",
    "        '''\n",
    "        with Chronometer() as chrono:\n",
    "            proba = np.empty((x.shape[0], len(self.prior)))\n",
    "            proba[:, :] = np.log(self.prior)\n",
    "\n",
    "            # np.nonzeros allows to consider only non-zero terms\n",
    "            self.condprobe = self.condprobe.T\n",
    "            for c, t in np.transpose(np.nonzero(x)):\n",
    "                proba[c] += np.log(self.condprobe[t])\n",
    "            \n",
    "        print('Predict in {:.3f}s'.format(float(chrono)))\n",
    "\n",
    "        return np.argmax(proba, axis=1)\n",
    "\n",
    "    def score(self, x, y):\n",
    "        return np.mean(self.predict(x) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count in 89.518s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count in 86.124s\n"
     ]
    }
   ],
   "source": [
    "X_ignore_true = count_words(texts, ignore_sw=True)\n",
    "X_ignore_false = count_words(texts, ignore_sw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cross_val(naive_bayes, x, cv=5):\n",
    "    return np.mean(cross_val_score(naive_bayes, x, y, cv=cv))\n",
    "\n",
    "def eval_nb(clf):\n",
    "    nb = clf()\n",
    "    print('\\nScore using ' + clf.__name__)\n",
    "    print('\\tignore_sw=False =>', cross_val(nb, X_ignore_false))\n",
    "    print('\\tignore_sw=True =>', cross_val(nb, X_ignore_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score using NB\n",
      "Fit in 0.149s\n",
      "Predict in 0.748s\n",
      "Fit in 0.149s\n",
      "Predict in 0.745s\n",
      "Fit in 0.147s\n",
      "Predict in 0.778s\n",
      "Fit in 0.148s\n",
      "Predict in 0.786s\n",
      "Fit in 0.146s\n",
      "Predict in 0.766s\n",
      "\tignore_sw=False => 0.8175\n",
      "Fit in 0.147s\n",
      "Predict in 0.549s\n",
      "Fit in 0.146s\n",
      "Predict in 0.544s\n",
      "Fit in 0.146s\n",
      "Predict in 0.571s\n",
      "Fit in 0.146s\n",
      "Predict in 0.580s\n",
      "Fit in 0.145s\n",
      "Predict in 0.558s\n",
      "\tignore_sw=True => 0.825\n"
     ]
    }
   ],
   "source": [
    "eval_nb(NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score using MultinomialNB\n",
      "\tignore_sw=False => 0.8095\n",
      "\tignore_sw=True => 0.806\n"
     ]
    }
   ],
   "source": [
    "eval_nb(MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(lowercase=True, stop_words={'english'})\n",
    "\n",
    "\n",
    "def eval_pipeline_with_params(clf, va, ngr):\n",
    "    pipeline = Pipeline([('vect', vect), ('clf', clf())])\n",
    "    pipeline.set_params(vect__analyzer=va, vect__ngram_range=ngr)\n",
    "    print(clf.__name__, va, ngr, cross_val(pipeline, texts))\n",
    "    \n",
    "\n",
    "\n",
    "def eval_pipeline(clf):\n",
    "    params = [(clf, va, ngr) \n",
    "          for va in ['word', 'char', 'char_wb']\n",
    "          for ngr in [(1, 1), (1, 2)]]\n",
    "    with Pool() as p:\n",
    "        p.starmap(eval_pipeline_with_params, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB word (1, 1) 0.8125\n",
      "MultinomialNB char (1, 1) 0.6095\n",
      "MultinomialNB word (1, 2) 0.8305\n",
      "MultinomialNB char_wb (1, 1) 0.6115\n",
      "MultinomialNB char (1, 2) 0.674\n",
      "MultinomialNB char_wb (1, 2) 0.6745\n"
     ]
    }
   ],
   "source": [
    "eval_pipeline(MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC word (1, 1) 0.8335\n",
      "LinearSVC char (1, 1) 0.518\n",
      "LinearSVC char_wb (1, 1) 0.5165\n",
      "LinearSVC word (1, 2) 0.85\n",
      "LinearSVC char (1, 2) 0.687\n",
      "LinearSVC char_wb (1, 2) 0.6785\n"
     ]
    }
   ],
   "source": [
    "eval_pipeline(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression word (1, 1) 0.842\n",
      "LogisticRegression char (1, 1) 0.6365\n",
      "LogisticRegression word (1, 2) 0.852\n",
      "LogisticRegression char_wb (1, 1) 0.637\n",
      "LogisticRegression char (1, 2) 0.705\n",
      "LogisticRegression char_wb (1, 2) 0.7035\n"
     ]
    }
   ],
   "source": [
    "eval_pipeline(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english', ignore_stopwords=False)\n",
    "vect_stem = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=sw,\n",
    "    tokenizer=lambda t: [stemmer.stem(token) for token in word_tokenize(t)],\n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='word'\n",
    ")\n",
    "\n",
    "def eval_stem(clf):\n",
    "    pipeline = Pipeline([('vect', vect_stem), ('clf', clf())])\n",
    "    print(clf.__name__, ':', cross_val(pipeline, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB : 0.8115\n"
     ]
    }
   ],
   "source": [
    "eval_stem(MultinomialNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.837\n"
     ]
    }
   ],
   "source": [
    "eval_stem(LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC : 0.8315\n"
     ]
    }
   ],
   "source": [
    "eval_stem(LinearSVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ok_pos = [\n",
    "    'JJ', 'JJR', 'JJS',                         # ADJECTIVES\n",
    "    'NN', 'NNP', 'NNPS',                        # NOUNS\n",
    "    'RB', 'RBR', 'RBS',                         # ADVERBS\n",
    "    'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'     # VERBS\n",
    "]\n",
    "\n",
    "\n",
    "def filter_words(t):\n",
    "    return [w for w, pos in pos_tag(word_tokenize(t)) if pos in ok_pos]\n",
    "\n",
    "vect_pos = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words=sw,\n",
    "    tokenizer=filter_words,\n",
    "    ngram_range=(1, 2),\n",
    "    analyzer='word'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression : 0.8355\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([('vect', vect_pos), ('clf', LogisticRegression())])\n",
    "print(LogisticRegression.__name__, ':', cross_val(pipeline, texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('vect', vect_pos),\n",
    "        ('tfidf', TfidfTransformer(sublinear_tf=True)),\n",
    "        ('clf', LogisticRegression())])\n",
    "print(LogisticRegression.__name__, ':', cross_val(pipeline, texts))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}