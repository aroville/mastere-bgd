import org.apache.spark.sql.SparkSession
import org.apache.spark.SparkConf
def appName = "Accidents"
val mySession = SparkSession.builder().config(new SparkConf().setAppName(appName)).getOrCreate()
val path = "/home/bigdata/spark-notebook-0.7.0/notebooks/telecom2016/data/"
val lieux = mySession.read.option("header", "true").format("com.databricks.spark.csv").load(path + "lieux_2015.csv").cache
val vehicules = mySession.read.option("header", "true").format("com.databricks.spark.csv").load(path + "vehicules_2015.csv").cache
val caracteristiques = mySession.read.option("header", "true").format("com.databricks.spark.csv").load(path + "caracteristiques_2015.csv").cache
val usagers = mySession.read.option("header", "true").format("com.databricks.spark.csv").load(path + "usagers_2015.csv").cache
val victims = usagers.groupBy("grav").count()
victims.map { row => { val mapping = Map("1" -> "Indemne", "2" -> "Tué", "3" -> "Blessé hospitalisé", "4" -> "Blessé léger"); mapping(row.getAs[String]("grav")) + " : " + row.getAs[String]("count") } }
import org.apache.spark.sql.types._
val usagers1 = usagers.withColumn("an_nais_int", col("an_nais").cast(IntegerType))
val computeAge = udf { (an_nais: Int) => 2016 - an_nais }
val usagersAge = usagers1.withColumn("age", computeAge(col("an_nais_int")))
val accidents = usagers.
filter($"grav" === 2).
groupBy("Num_Acc").
count().
sort(desc("count")).
limit(3).
withColumnRenamed("count", "nb_victimes").
join(caracteristiques, usagers("Num_Acc") === caracteristiques("Num_Acc")).
join(lieux, usagers("Num_Acc") === lieux("Num_Acc")).
join(vehicules, usagers("Num_Acc") === vehicules("Num_Acc")).
groupBy(caracteristiques("an"), caracteristiques("mois"), caracteristiques("jour"), caracteristiques("adr"), caracteristiques("dep"), caracteristiques("atm"), $"nb_victimes").
count().withColumnRenamed("count", "nb_vehicules")
case class Caracteristiques(NumAcc: String, latitude: Double, longitude: Double, adr: String, colType: Int)
import org.apache.spark.sql.Row
def mapRow(r:Row): Caracteristiques = {
Caracteristiques(
r.getAs[String]("Num_Acc"),
r.getAs[String]("lat").toDouble / 100000,
r.getAs[String]("long").toDouble / 100000,
r.getAs[String]("adr"),
r.getAs[String]("col").toInt)
}
val caractDS = caracteristiques.filter(
x=> {
try {
mapRow(x)
true
} catch {
case e: Exception => false
}
}).map { mapRow }
